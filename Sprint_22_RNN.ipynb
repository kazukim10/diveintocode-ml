{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ　リカレンとニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#このSprintについて\" data-toc-modified-id=\"このSprintについて-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>このSprintについて</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sprintの目的\" data-toc-modified-id=\"Sprintの目的-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Sprintの目的</a></span></li><li><span><a href=\"#どのように学ぶか\" data-toc-modified-id=\"どのように学ぶか-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>どのように学ぶか</a></span></li></ul></li><li><span><a href=\"#リカレントニューラルネットワークスクラッチ\" data-toc-modified-id=\"リカレントニューラルネットワークスクラッチ-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>リカレントニューラルネットワークスクラッチ</a></span><ul class=\"toc-item\"><li><span><a href=\"#【問題1】SimpleRNNのフォワードプロパゲーション実装\" data-toc-modified-id=\"【問題1】SimpleRNNのフォワードプロパゲーション実装-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>【問題1】SimpleRNNのフォワードプロパゲーション実装</a></span></li><li><span><a href=\"#【問題2】小さな配列でのフォワードプロパゲーションの実験\" data-toc-modified-id=\"【問題2】小さな配列でのフォワードプロパゲーションの実験-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>【問題2】小さな配列でのフォワードプロパゲーションの実験</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## このSprintについて\n",
    "### Sprintの目的\n",
    "* スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチでリカレントニューラルネットワークの実装を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リカレントニューラルネットワークスクラッチ\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \"\"\"\n",
    "    活性化関数 : ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        self.Z = np.tanh(self.A)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        return dZ*(1-self.Z**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "        \n",
    "    def Wx(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重み\n",
    "        \"\"\"\n",
    "        #self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        Wx = np.array([[1, 3, 5, 7],\n",
    "                       [3, 5, 7, 8]])/100 # 問題2のミニテスト用Wx\n",
    "        return Wx\n",
    "    \n",
    "    def Wh(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重み\n",
    "        \"\"\"\n",
    "        #self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        Wh = np.array([[1, 3, 5, 7],\n",
    "                       [2, 4, 6, 8],\n",
    "                       [3, 5, 7, 8],\n",
    "                       [4, 6, 8, 10]])/100 # 問題2のミニテスト用Wh\n",
    "        return Wh\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : バイアス\n",
    "        \"\"\"\n",
    "        #np.zeros(n_nodes2)\n",
    "        B = np.array([1, 1, 1, 1]) # 問題2のミニテスト用バイアス\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_batch, input_nodes, output_nodes, initializer, activation):\n",
    "        \n",
    "        self.n_batch = n_batch\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.initializer = initializer\n",
    "        self.activation = activation\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドで、self.Wx,self.Wh,self.Bを初期化する\n",
    "        \n",
    "        self.Wx = self.initializer.Wx(self.input_nodes, self.output_nodes)\n",
    "        self.Wh = self.initializer.Wh(self.output_nodes, self.output_nodes)\n",
    "        self.B = self.initializer.B(self.output_nodes)\n",
    "        \n",
    "        self.H = np.zeros([self.n_batch, self.output_nodes])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.A = np.dot(self.X,self.Wx) +np.dot(self.H,self.Wh) +self.B\n",
    "        self.H = self.activation.forward(self.A)\n",
    "        \n",
    "        return self.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleRNNClassifier:\n",
    "    \n",
    "    def __init__(self, n_nodes):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.n_nodes = n_nodes\n",
    "        \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.n_batch = X.shape[0]\n",
    "        self.n_features = X.shape[2]\n",
    "        \n",
    "        initializer = SimpleInitializer()\n",
    "        \n",
    "        affine = FC(self.n_batch, self.n_features, self.n_nodes, initializer, Tanh())\n",
    "        \n",
    "        # forward\n",
    "        for t in range(X.shape[1]):\n",
    "            h = affine.forward(X[:,t,:])\n",
    "            \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, n_sequences, n_features)\n",
    "x = np.array([[[1, 2],\n",
    "               [2, 3],\n",
    "               [3, 4]]])/100 \n",
    "\n",
    " # (n_features, n_nodes)\n",
    "w_x = np.array([[1, 3, 5, 7],\n",
    "                [3, 5, 7, 8]])/100\n",
    "\n",
    "# (n_nodes, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7],\n",
    "                [2, 4, 6, 8],\n",
    "                [3, 5, 7, 8],\n",
    "                [4, 6, 8, 10]])/100 \n",
    "\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "rnn = ScratchSimpleRNNClassifier(4)\n",
    "print(rnn.fit(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
