{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 20 セグメンテーション2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#このSprintについて\" data-toc-modified-id=\"このSprintについて-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>このSprintについて</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sprintの目的\" data-toc-modified-id=\"Sprintの目的-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Sprintの目的</a></span></li><li><span><a href=\"#どのように学ぶか\" data-toc-modified-id=\"どのように学ぶか-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>どのように学ぶか</a></span></li></ul></li><li><span><a href=\"#セグメンテーションの精度向上\" data-toc-modified-id=\"セグメンテーションの精度向上-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>セグメンテーションの精度向上</a></span><ul class=\"toc-item\"><li><span><a href=\"#【問題1】コードレビュー\" data-toc-modified-id=\"【問題1】コードレビュー-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>【問題1】コードレビュー</a></span></li><li><span><a href=\"#IMPORT\" data-toc-modified-id=\"IMPORT-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>IMPORT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-loading-&amp;-depth-merge:¶\" data-toc-modified-id=\"Data-loading-&amp;-depth-merge:¶-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Data loading &amp; depth merge:¶</a></span></li></ul></li><li><span><a href=\"#Compute-salt-coverage-(this-will-serve-as-a-basis-for-stratified-split):\" data-toc-modified-id=\"Compute-salt-coverage-(this-will-serve-as-a-basis-for-stratified-split):-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Compute salt coverage (this will serve as a basis for stratified split):</a></span></li><li><span><a href=\"#Prepare-data-for-training:\" data-toc-modified-id=\"Prepare-data-for-training:-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Prepare data for training:</a></span></li><li><span><a href=\"#Encoder-features---ResNet50:\" data-toc-modified-id=\"Encoder-features---ResNet50:-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Encoder features - ResNet50:</a></span></li><li><span><a href=\"#Decoder-blocks:\" data-toc-modified-id=\"Decoder-blocks:-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Decoder blocks:</a></span></li><li><span><a href=\"#Model-definition:\" data-toc-modified-id=\"Model-definition:-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Model definition:</a></span></li><li><span><a href=\"#Inspect-created-model:\" data-toc-modified-id=\"Inspect-created-model:-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Inspect created model:</a></span></li><li><span><a href=\"#Train-model:\" data-toc-modified-id=\"Train-model:-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Train model:</a></span></li><li><span><a href=\"#【問題2】コードの書き換え\" data-toc-modified-id=\"【問題2】コードの書き換え-2.10\"><span class=\"toc-item-num\">2.10&nbsp;&nbsp;</span>【問題2】コードの書き換え</a></span></li><li><span><a href=\"#【問題3】学習・推定\" data-toc-modified-id=\"【問題3】学習・推定-2.11\"><span class=\"toc-item-num\">2.11&nbsp;&nbsp;</span>【問題3】学習・推定</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "* セグメンテーションの精度を向上させる\n",
    "\n",
    "### どのように学ぶか\n",
    "Kaggleコンペティションの情報を参考にセグメンテーションの精度を向上させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セグメンテーションの精度向上\n",
    "\n",
    "前回に引き続きTGS Salt Identification Challengのデータセットの学習・推定を行います。\n",
    "\n",
    "TGS Salt Identification Challenge | Kaggle  \n",
    "https://www.kaggle.com/c/tgs-salt-identification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】コードレビュー\n",
    "転移学習を使用してセグメンテーションの精度を改善したコードを提示するので、レビューを行ってください。\n",
    "\n",
    "《視点例》\n",
    "\n",
    "* 前回使用した実装とはどのように違うのか\n",
    "* 転移学習をどのように行っているか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* imagenetで学習したResNetのパラメータを使用している。（前回は、U-Netのみで初期値も設定無し）\n",
    "* ResNetのパラメータも微調整している（エポック2）、追加層のみ学習（ResNetの凍結）はしていない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.callbacks import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage(df, masks):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    def cov_to_class(val):\n",
    "        for i in range(0, 11):\n",
    "            if val * 10 <= i:\n",
    "                return i\n",
    "\n",
    "    # Output percentage of area covered by class\n",
    "    df['coverage'] = np.mean(masks, axis=(1, 2))\n",
    "    # Coverage must be split into bins, otherwise stratified split will not be possible,\n",
    "    # because each coverage will occur only once.\n",
    "    df['coverage_class'] = df.coverage.map(\n",
    "        cov_to_class)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_depth_abs_channels(image_tensor):\n",
    "    image_tensor = image_tensor.astype(np.float32)\n",
    "    h, w, c = image_tensor.shape\n",
    "    for row, const in enumerate(np.linspace(0, 1, h)):\n",
    "        image_tensor[row, :, 1] = const\n",
    "    image_tensor[:, :, 2] = (\n",
    "        image_tensor[:, :, 0] * image_tensor[:, :, 1])\n",
    "\n",
    "    x_dx = np.diff(image_tensor[:, :, 0], axis=0)\n",
    "    x_dy = np.diff(image_tensor[:, :, 0], axis=1)\n",
    "    x_dx = cv2.copyMakeBorder(x_dx, 1, 0, 0, 0, cv2.BORDER_CONSTANT, 0)\n",
    "    x_dy = cv2.copyMakeBorder(x_dy, 0, 0, 1, 0, cv2.BORDER_CONSTANT, 0)\n",
    "    image_tensor[:, :, 1] = np.abs(x_dx + x_dy)\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading & depth merge:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "           id                                           rle_mask\n",
      "0  575d24d81d                                                NaN\n",
      "1  a266a2a9df                                          5051 5151\n",
      "2  75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...\n",
      "3  34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...\n",
      "4  4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...\n",
      "\n",
      "test:\n",
      "           id rle_mask\n",
      "0  155410d6fa      1 1\n",
      "1  78b32781d1      1 1\n",
      "2  63db2a476a      1 1\n",
      "3  17bfcdb967      1 1\n",
      "4  7ea0fd3c88      1 1\n",
      "\n",
      "           id                                           rle_mask    z\n",
      "0  575d24d81d                                                NaN  843\n",
      "1  a266a2a9df                                          5051 5151  794\n",
      "2  75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...  468\n",
      "3  34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...  727\n",
      "4  4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...  797\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./tgs-salt-identification-challenge/train.csv')\n",
    "test = pd.read_csv('./tgs-salt-identification-challenge/sample_submission.csv')\n",
    "depth = pd.read_csv('./tgs-salt-identification-challenge/depths.csv')\n",
    "\n",
    "train_src = './tgs-salt-identification-challenge/train/'\n",
    "\n",
    "print('train:\\n{}'.format(train.head()))\n",
    "print('\\ntest:\\n{}'.format(test.head()))\n",
    "\n",
    "\n",
    "train = train.merge(depth, how='left', on='id')\n",
    "test = test.merge(depth, how='left', on='id')\n",
    "\n",
    "print('\\n{}'.format(train.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 101, 101) (4000, 101, 101)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.asarray(\n",
    "    [cv2.imread('./tgs-salt-identification-challenge/train/images/{}.png'.format(x), 0) for x in train.id.tolist()], \n",
    "    dtype=np.uint8) / 255. # unit.8\n",
    "y_train = np.asarray(\n",
    "    [cv2.imread('./tgs-salt-identification-challenge/train/masks/{}.png'.format(x), 0) for x in train.id.tolist()],\n",
    "    dtype=np.uint8) / 255. # unit.8\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.randint(0, X_train.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].imshow(X_train[random_index], cmap='gray')\n",
    "ax[1].imshow(y_train[random_index], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute salt coverage (this will serve as a basis for stratified split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = compute_coverage(train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kazuki/.pyenv/versions/anaconda3-2020.02/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 224, 224, 3) (3200, 224, 224, 1)\n",
      "(800, 224, 224, 3) (800, 224, 224, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5028"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=1337)\n",
    "\n",
    "# Add channel features\n",
    "X_train_ch = np.repeat(np.expand_dims(X_train, axis=-1), 3, -1)\n",
    "X_train_ch = np.asarray(list(map(lambda x: create_depth_abs_channels(x), X_train_ch)))\n",
    "\n",
    "# Resize to 224x224, default ResNet50 image size\n",
    "X_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), X_train_ch)))\n",
    "y_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), y_train)))\n",
    "\n",
    "\n",
    "for train_index, valid_index in kfold.split(train.id.values, train.coverage_class.values):\n",
    "    \n",
    "    X_tr, X_val = X_resized[train_index], X_resized[valid_index]\n",
    "    y_tr, y_val = y_resized[train_index], y_resized[valid_index]\n",
    "    \n",
    "    break\n",
    "    \n",
    "\n",
    "y_tr = np.expand_dims(y_tr, axis=-1)\n",
    "y_val = np.expand_dims(y_val, axis=-1)\n",
    "\n",
    "print(X_tr.shape, y_tr.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "\n",
    "del X_train_ch, y_resized\n",
    "del X_resized\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "\n",
    "# Dice & combined\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# Lovash loss: https://github.com/bermanmaxim/LovaszSoftmax\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "def lovasz_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
    "    #logits = K.log(y_pred / (1. - y_pred))\n",
    "    logits = y_pred #Jiaxin\n",
    "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# IoU metric for observation during training\n",
    "# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\n",
    "def get_iou_vector(A, B):\n",
    "    # Numpy version    \n",
    "    batch_size = A.shape[0]\n",
    "    metric = 0.0\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch], B[batch]\n",
    "        true = np.sum(t)\n",
    "        pred = np.sum(p)\n",
    "        \n",
    "        # deal with empty mask first\n",
    "        if true == 0:\n",
    "            metric += (pred == 0)\n",
    "            continue\n",
    "        \n",
    "        # non empty mask case.  Union is never empty \n",
    "        # hence it is safe to divide by its number of pixels\n",
    "        intersection = np.sum(t * p)\n",
    "        union = true + pred - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # iou metrric is a stepwise approximation of the real iou over 0.5\n",
    "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
    "        \n",
    "        metric += iou\n",
    "        \n",
    "    # teake the average over all images in batch\n",
    "    metric /= batch_size\n",
    "    return metric\n",
    "\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_function(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "\n",
    "\n",
    "# For Lovash loss\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return tf.py_function(get_iou_vector, [label, pred >0], tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder features - ResNet50:\n",
    "\n",
    "In ResNet50, each block finishes with a pooling layer, so we can extract features from intermediate layers just before the pooling. This way, when first layer is added as additional extractor, we will have features extracted from 5 layers. Default input size will be assumed, which is (224, 224, 3). Layers will be as follows:\n",
    "\n",
    "* 'activation_1', shape: (None, 112, 112, 64)\n",
    "* 'activation_10', shape: (None, 56, 56, 256)\n",
    "* 'activation_22', shape: (None, 28, 28, 512)\n",
    "* 'activation_40', shape: (None, 14, 14, 1024)\n",
    "* 'activation_49', shape: (None, 7, 7, 2048)\n",
    "\n",
    "One thing to keep in mind is that every time a model will be created in the same TF session in the notebook, layer names will change, so above layer names correspond to first creation of the model. In order to reset session, call K.clear_session()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_size = (224, 224, 3)\n",
    "\n",
    "base_model = ResNet50(input_shape=input_size, include_top=False)\n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder blocks:\n",
    "Features from ResNet50 will serve as a basis for encoder part of the segmentation model, now a decoder part is needed. For this part, we will have to create our own blocks. Let's create a very basic block and a second one, which structure will have a more complicated structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic decoder block with Conv, BN and PReLU activation.\n",
    "def decoder_block_simple(\n",
    "        layer_name, block_name,\n",
    "        num_filters=32,\n",
    "        conv_dim=(3, 3)):\n",
    "\n",
    "    x_dec = Conv2D(\n",
    "        num_filters, conv_dim,\n",
    "        padding='same',\n",
    "        name='{}_conv'.format(block_name))(layer_name)\n",
    "    x_dec = BatchNormalization(\n",
    "        name='{}_bn'.format(block_name))(x_dec)\n",
    "    x_dec = PReLU(\n",
    "        name='{}_activation'.format(block_name))(x_dec)\n",
    "\n",
    "    return x_dec\n",
    "\n",
    "# Decoder block with bottleneck architecture, where middle conv layer\n",
    "# is half the size of first and last, in order to compress representation.\n",
    "# This type of architecture is supposed to retain most useful information.\n",
    "def decoder_block_bottleneck(\n",
    "        layer_name, block_name,\n",
    "        num_filters=32,\n",
    "        conv_dim=(3, 3),\n",
    "        dropout_frac=0.2):\n",
    "\n",
    "    x_dec = Conv2D(\n",
    "        num_filters, conv_dim,\n",
    "        padding='same',\n",
    "        name='{}_conv1'.format(block_name))(layer_name)\n",
    "    x_dec = BatchNormalization(\n",
    "        name='{}_bn1'.format(block_name))(x_dec)\n",
    "    x_dec = PReLU(\n",
    "        name='{}_activation1'.format(block_name))(x_dec)\n",
    "    x_dec = Dropout(dropout_frac)(x_dec)\n",
    "\n",
    "    x_dec2 = Conv2D(\n",
    "        num_filters // 2, conv_dim,\n",
    "        padding='same',\n",
    "        name='{}_conv2'.format(block_name))(x_dec)\n",
    "    x_dec2 = BatchNormalization(\n",
    "        name='{}_bn2'.format(block_name))(x_dec2)\n",
    "    x_dec2 = PReLU(\n",
    "        name='{}_activation2'.format(block_name))(x_dec2)\n",
    "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
    "\n",
    "    x_dec2 = Conv2D(\n",
    "        num_filters, conv_dim,\n",
    "        padding='same',\n",
    "        name='{}_conv3'.format(block_name))(x_dec2)\n",
    "    x_dec2 = BatchNormalization(\n",
    "        name='{}_bn3'.format(block_name))(x_dec2)\n",
    "    x_dec2 = PReLU(\n",
    "        name='{}_activation3'.format(block_name))(x_dec2)\n",
    "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
    "\n",
    "    x_dec2 = Add()([x_dec, x_dec2])\n",
    "\n",
    "    return x_dec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition:\n",
    "Combine encoder and decoder blocks to create final segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is parametrized in a way to enable easy change of decoder_block type,\n",
    "# as this is an argument that can be given a function, like decoder_block_simple.\n",
    "def unet_resnet(input_size,\n",
    "                decoder_block,\n",
    "                weights='imagenet',\n",
    "                loss_func='binary_crossentropy',\n",
    "                metrics_list=[my_iou_metric],\n",
    "                use_lovash=False):\n",
    "\n",
    "    # Base model - encoder\n",
    "    base_model = ResNet50(\n",
    "        input_shape=input_size, \n",
    "        include_top=False,\n",
    "        weights=weights)\n",
    "    \n",
    "    # Layers for feature extraction in the encoder part\n",
    "    encoder1 = base_model.get_layer('conv1_conv').output # activation_1\n",
    "    encoder2 = base_model.get_layer('conv2_block3_3_conv').output # activation_10\n",
    "    encoder3 = base_model.get_layer('conv3_block4_3_conv').output # activation_22\n",
    "    encoder4 = base_model.get_layer('conv4_block5_3_conv').output # activation_40\n",
    "    encoder5 = base_model.get_layer('conv5_block2_3_conv').output # activation_40\n",
    "\n",
    "    # Center block\n",
    "    center = decoder_block(\n",
    "        encoder5, 'center', num_filters=512)\n",
    "    concat5 = concatenate([center, encoder5], axis=-1)\n",
    "\n",
    "    # Decoder part.\n",
    "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
    "    # This creates skip connections.\n",
    "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
    "    decoder4 = decoder_block(\n",
    "        concat5, 'decoder4', num_filters=256)\n",
    "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
    "\n",
    "    decoder3 = decoder_block(\n",
    "        concat4, 'decoder3', num_filters=128)\n",
    "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
    "\n",
    "    decoder2 = decoder_block(\n",
    "        concat3, 'decoder2', num_filters=64)\n",
    "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
    "\n",
    "    decoder1 = decoder_block(\n",
    "        concat2, 'decoder1', num_filters=64)\n",
    "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
    "\n",
    "    # Final upsampling and decoder block for segmentation.\n",
    "    output = UpSampling2D()(concat1)\n",
    "    output = decoder_block(\n",
    "        output, 'decoder_output', num_filters=32)\n",
    "    output = Conv2D(\n",
    "        1, (1, 1), activation=None, name='prediction')(output)\n",
    "    if not use_lovash:\n",
    "        output = Activation('sigmoid')(output)\n",
    "        \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    #model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import tf.compat.v1.keras.backend as K\n",
    "\n",
    "input_size = (224, 224, 3)\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = unet_resnet(input_size,\n",
    "                    decoder_block_simple,\n",
    "                    weights='imagenet')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 32 samples\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.4371 - acc: 0.8182 WARNING:tensorflow:From /Users/kazuki/.pyenv/versions/anaconda3-2020.02/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "320/320 [==============================] - 475s 1s/sample - loss: 0.4371 - acc: 0.8182 - val_loss: 11.0478 - val_acc: 0.2694\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build model:\n",
    "# Here, you can experiment with various losses.\n",
    "# For dice and BCE (binary_crossentropy), my_iou_metric should be used,\n",
    "# whereas for lovash_loss my_iou_metric2 should be used, because range of values\n",
    "# for lovash loss is between -inf and +inf, not between 0 and 1, as for BCE and dice.\n",
    "# What is more, when lovash loss is used, last layer (sigmoid) should be deleted.\n",
    "# This is controlled by use_lovash parameter.\n",
    "\n",
    "model_depth = unet_resnet(input_size, \n",
    "                          decoder_block_simple, # bottle\n",
    "                          weights='imagenet',\n",
    "                          #loss_func=bce_dice_loss, \n",
    "                          #metrics_list=[my_iou_metric],\n",
    "                          use_lovash=False)\n",
    "\n",
    "#print(model_depth.summary())\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('unet_resnet.h5',\n",
    "                                   monitor='val_my_iou_metric', \n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric',\n",
    "                              mode='max',\n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=1)\n",
    "\n",
    "epochs = 1  # 25\n",
    "batch_size = 16\n",
    "\n",
    "X_tr = X_tr.astype('float32')\n",
    "y_tr = y_tr.astype('float32')\n",
    "\n",
    "history = model_depth.fit(X_tr[:320],\n",
    "                          y_tr[:320],\n",
    "                          validation_data=[X_val[:32], y_val[:32]], \n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          #callbacks=[model_checkpoint,reduce_lr], \n",
    "                          verbose=1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】コードの書き換え\n",
    "エンコーダーにResNetが使用されていたコードをVGGに変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "def unet_vgg(input_size,\n",
    "                decoder_block,\n",
    "                weights='imagenet',\n",
    "                loss_func='binary_crossentropy',\n",
    "                metrics_list=[my_iou_metric],\n",
    "                use_lovash=False):\n",
    "\n",
    "    # Base model - encoder\n",
    "    base_model = VGG16(\n",
    "        input_shape=input_size, \n",
    "        include_top=False,\n",
    "        weights=weights)\n",
    "    \n",
    "    # Layers for feature extraction in the encoder part\n",
    "    encoder1 = base_model.get_layer('conv1_conv').output # activation_1\n",
    "    encoder2 = base_model.get_layer('conv2_block3_3_conv').output # activation_10\n",
    "    encoder3 = base_model.get_layer('conv3_block4_3_conv').output # activation_22\n",
    "    encoder4 = base_model.get_layer('conv4_block5_3_conv').output # activation_40\n",
    "    encoder5 = base_model.get_layer('conv5_block2_3_conv').output # activation_40\n",
    "\n",
    "    # Center block\n",
    "    center = decoder_block(\n",
    "        encoder5, 'center', num_filters=512)\n",
    "    concat5 = concatenate([center, encoder5], axis=-1)\n",
    "\n",
    "    # Decoder part.\n",
    "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
    "    # This creates skip connections.\n",
    "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
    "    decoder4 = decoder_block(\n",
    "        concat5, 'decoder4', num_filters=256)\n",
    "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
    "\n",
    "    decoder3 = decoder_block(\n",
    "        concat4, 'decoder3', num_filters=128)\n",
    "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
    "\n",
    "    decoder2 = decoder_block(\n",
    "        concat3, 'decoder2', num_filters=64)\n",
    "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
    "\n",
    "    decoder1 = decoder_block(\n",
    "        concat2, 'decoder1', num_filters=64)\n",
    "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
    "\n",
    "    # Final upsampling and decoder block for segmentation.\n",
    "    output = UpSampling2D()(concat1)\n",
    "    output = decoder_block(\n",
    "        output, 'decoder_output', num_filters=32)\n",
    "    output = Conv2D(\n",
    "        1, (1, 1), activation=None, name='prediction')(output)\n",
    "    if not use_lovash:\n",
    "        output = Activation('sigmoid')(output)\n",
    "        \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    #model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 32 samples\n",
      "320/320 [==============================] - 492s 2s/sample - loss: 0.5288 - acc: 0.7773 - val_loss: 11.0663 - val_acc: 0.2694\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build model:\n",
    "# Here, you can experiment with various losses.\n",
    "# For dice and BCE (binary_crossentropy), my_iou_metric should be used,\n",
    "# whereas for lovash_loss my_iou_metric2 should be used, because range of values\n",
    "# for lovash loss is between -inf and +inf, not between 0 and 1, as for BCE and dice.\n",
    "# What is more, when lovash loss is used, last layer (sigmoid) should be deleted.\n",
    "# This is controlled by use_lovash parameter.\n",
    "\n",
    "model_depth = unet_resnet(input_size, \n",
    "                          decoder_block_simple, # bottle\n",
    "                          weights='imagenet',\n",
    "                          #loss_func=bce_dice_loss, \n",
    "                          #metrics_list=[my_iou_metric],\n",
    "                          use_lovash=False)\n",
    "\n",
    "#print(model_depth.summary())\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('unet_resnet.h5',\n",
    "                                   monitor='val_my_iou_metric', \n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric',\n",
    "                              mode='max',\n",
    "                              factor=0.5, \n",
    "                              patience=5, \n",
    "                              min_lr=0.0001, \n",
    "                              verbose=1)\n",
    "\n",
    "epochs = 1  # 25\n",
    "batch_size = 16\n",
    "\n",
    "X_tr = X_tr.astype('float32')\n",
    "y_tr = y_tr.astype('float32')\n",
    "\n",
    "history = model_depth.fit(X_tr[:320],\n",
    "                          y_tr[:320],\n",
    "                          validation_data=[X_val[:32], y_val[:32]], \n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          #callbacks=[model_checkpoint,reduce_lr], \n",
    "                          verbose=1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】学習・推定\n",
    "ResNetとVGG双方のコードで学習・推定を行い、結果を比較してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* データを1/10にして学習、推定を行った。\n",
    "* ResNetのレイヤーを使用した方が精度は高かった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
